{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.4.0+cu124', 'cuda available')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"тренируем автоэнкодер который работает на внутреннем представлениии кодера для зуба ( 256* 32), \n",
    "   метки выдает тот же кодер из T2.\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "from points_dataset import EmbedderDataset\n",
    "from alignment_ae_dataset import AlignerDataset\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models import *\n",
    "from models_align import *\n",
    "from point_cloud_dataset import PointCloudDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.__version__, \"cuda available\" if torch.cuda.is_available() else \"cpu only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\5010858\\AppData\\Local\\Temp/ipykernel_30280/1466985861.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  align_ds = torch.load(align_ds_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device, ds length - 957\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "\n",
    "model_dir = \"models_align\"\n",
    "align_ds_path = r\"datasets_align\\dataset_128.pth\"\n",
    "align_ds = torch.load(align_ds_path)\n",
    "# align_ds_data = align_ds.data # нет у конкатенрованного ds такого поля \n",
    "print(f\"Using {device} device, ds length - {len(align_ds)}\")\n",
    "\n",
    "EPOCHS = 560\n",
    "BATCH_SIZE = 64\n",
    "SPLIT_FACTOR = .8\n",
    "# TRAIN_MODE = (False, True)[1]\n",
    "# REMOVE_OLD_MODELS = True\n",
    "N_TEETH = 32\n",
    "POINTS_PER_SHAPE = 128 # 256 не лезет в cuda\n",
    "POINT_DIM = 3\n",
    "EMBEDDED_POINT_DIM = 2\n",
    "\n",
    "hidden_dim = POINTS_PER_SHAPE * EMBEDDED_POINT_DIM * N_TEETH // 6\n",
    "\n",
    "train_loader = DataLoader(align_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model = autoencoder(POINTS_PER_SHAPE*EMBEDDED_POINT_DIM*N_TEETH, hidden_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss 1.06e+06\n",
      "Epoch 10 - loss 57.889\n",
      "Epoch 20 - loss 57.421\n",
      "Epoch 30 - loss 57.197\n",
      "Epoch 40 - loss 59.942\n",
      "Epoch 50 - loss 57.792\n",
      "Epoch 60 - loss 63.957\n",
      "Epoch 70 - loss 57.117\n",
      "Epoch 80 - loss 67.767\n",
      "Epoch 90 - loss 57.077\n",
      "Epoch 100 - loss 56.99\n",
      "Epoch 110 - loss 57.23\n",
      "Epoch 120 - loss 57.421\n",
      "Epoch 130 - loss 56.155\n",
      "Epoch 140 - loss 56.11\n",
      "Epoch 150 - loss 56.062\n",
      "Epoch 160 - loss 56.043\n",
      "Epoch 170 - loss 56.058\n",
      "Epoch 180 - loss 55.967\n",
      "Epoch 190 - loss 56.368\n",
      "Epoch 200 - loss 55.944\n",
      "Epoch 210 - loss 55.87\n",
      "Epoch 220 - loss 55.912\n",
      "Epoch 230 - loss 55.774\n",
      "Epoch 240 - loss 55.927\n",
      "Epoch 250 - loss 55.716\n",
      "Epoch 260 - loss 55.661\n",
      "Epoch 270 - loss 55.573\n",
      "Epoch 280 - loss 55.517\n",
      "Epoch 290 - loss 55.496\n",
      "Epoch 300 - loss 55.364\n",
      "Epoch 310 - loss 55.376\n",
      "Epoch 320 - loss 55.314\n",
      "Epoch 330 - loss 55.304\n",
      "Epoch 340 - loss 55.188\n",
      "Epoch 350 - loss 55.13\n",
      "Epoch 360 - loss 55.038\n",
      "Epoch 370 - loss 55.056\n",
      "Epoch 380 - loss 55.204\n",
      "Epoch 390 - loss 54.881\n",
      "Epoch 400 - loss 55.271\n",
      "Epoch 410 - loss 54.83\n",
      "Epoch 420 - loss 54.876\n",
      "Epoch 430 - loss 55.18\n",
      "Epoch 440 - loss 54.636\n",
      "Epoch 450 - loss 54.554\n",
      "Epoch 460 - loss 5.139e+04\n",
      "Epoch 470 - loss 5.1318e+04\n",
      "Epoch 480 - loss 5.1196e+04\n",
      "Epoch 490 - loss 5.1065e+04\n",
      "Epoch 500 - loss 5.0924e+04\n",
      "Epoch 510 - loss 5.0773e+04\n",
      "Epoch 520 - loss 5.061e+04\n",
      "Epoch 530 - loss 5.0436e+04\n",
      "Epoch 540 - loss 5.0248e+04\n",
      "Epoch 550 - loss 5.0047e+04\n",
      "Best model saved at: None\n"
     ]
    }
   ],
   "source": [
    "def train_autoencoder(model, train_loader, device, epochs=EPOCHS, lr=1e-3, log_dir=\"runs/autoencoder\"):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_path = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # running_loss_val = 0\n",
    "        counter = 0\n",
    "        # counter_val = 0\n",
    "        \n",
    "        for batch_idx, (x,y) in enumerate(train_loader):\n",
    "            counter +=1\n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            x = x.view(-1, POINTS_PER_SHAPE*EMBEDDED_POINT_DIM*N_TEETH ).to(device)\n",
    "            y = y.view(-1, POINTS_PER_SHAPE*EMBEDDED_POINT_DIM*N_TEETH ).to(device)\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "             # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # decoder_optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss = epoch_loss / counter\n",
    "        # loss = loss.item()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "        # loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"Epoch {epoch} - loss {epoch_loss:.5}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "best_model_path = train_autoencoder(model, train_loader, device)\n",
    "print(f\"Best model saved at: {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262144, 480)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256*32*32, 15*32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
