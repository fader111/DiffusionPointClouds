{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.4.0+cu124', 'cuda available')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import ChebConv  # Using Chebyshev convolution\n",
    "from torch_geometric.nn import knn_graph\n",
    "from torch_geometric.utils import get_laplacian, get_mesh_laplacian\n",
    "# from torch_sparse import coalesce\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "from embedder_dataset import EmbedderDataset\n",
    "# from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "from models import *\n",
    "from misc import compute_edge_indices, compute_laplacian#, compute_batched_edge_indices\n",
    "from point_cloud_datasest import PointCloudDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.__version__, \"cuda available\" if torch.cuda.is_available() else \"cpu only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\5010858\\AppData\\Local\\Temp/ipykernel_16096/2273564222.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ds = torch.load(ds_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device, ds length - 27571, data.shape - (27571, 3072)\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "\n",
    "# ds_dir = \"datasets_embedded\"\n",
    "# ds_fname = \"ds_1024.pth\" # 1024 points per shape\n",
    "ds_path = r\"C:\\Projects\\multiSetup\\datasets_embedded\\ds_1024.pth\"\n",
    "ds = torch.load(ds_path)\n",
    "dataset_data = ds.data\n",
    "\n",
    "print(f\"Using {device} device, ds length - {len(ds)}, data.shape - {dataset_data.shape}\")\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "SPLIT_FACTOR = .8\n",
    "TRAIN_MODE = (False, True)[1]\n",
    "REMOVE_OLD_MODELS = True\n",
    "POINTS_PER_SHAPE = 1024\n",
    "POINT_DIM = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "diffision_ae_ds = PointCloudDataset(torch.from_numpy(dataset_data), k=6)\n",
    "len(diffision_ae_ds)\n",
    "train_loader = DataLoader(diffision_ae_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_loader))\n",
    "# batch # DataBatch(x=[131072, 3], edge_index=[2, 786432], edge_weight=[786432], batch=[131072], ptr=[129])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Loss: 10.850329862700569\n",
      "Best model saved with loss 10.850329862700569 at epoch 0\n",
      "Epoch 1/1000, Loss: 10.330636854524966\n",
      "Best model saved with loss 10.330636854524966 at epoch 1\n",
      "Epoch 2/1000, Loss: 9.666301153324268\n",
      "Best model saved with loss 9.666301153324268 at epoch 2\n",
      "Epoch 3/1000, Loss: 9.605218525286075\n",
      "Best model saved with loss 9.605218525286075 at epoch 3\n",
      "Epoch 4/1000, Loss: 9.581684558479875\n",
      "Best model saved with loss 9.581684558479875 at epoch 4\n",
      "Epoch 5/1000, Loss: 9.572846112427888\n",
      "Best model saved with loss 9.572846112427888 at epoch 5\n",
      "Epoch 6/1000, Loss: 9.570757159480342\n",
      "Best model saved with loss 9.570757159480342 at epoch 6\n",
      "Epoch 7/1000, Loss: 9.561217723069367\n",
      "Best model saved with loss 9.561217723069367 at epoch 7\n",
      "Epoch 8/1000, Loss: 9.560262596165693\n",
      "Best model saved with loss 9.560262596165693 at epoch 8\n",
      "Epoch 9/1000, Loss: 9.55987505559568\n",
      "Best model saved with loss 9.55987505559568 at epoch 9\n",
      "Epoch 10/1000, Loss: 9.55373692954028\n",
      "Best model saved with loss 9.55373692954028 at epoch 10\n",
      "Epoch 11/1000, Loss: 9.553700738483005\n",
      "Best model saved with loss 9.553700738483005 at epoch 11\n",
      "Epoch 12/1000, Loss: 9.553160018391079\n",
      "Best model saved with loss 9.553160018391079 at epoch 12\n",
      "Epoch 13/1000, Loss: 9.550928186487269\n",
      "Best model saved with loss 9.550928186487269 at epoch 13\n",
      "Epoch 14/1000, Loss: 9.550658799983838\n",
      "Best model saved with loss 9.550658799983838 at epoch 14\n",
      "Epoch 15/1000, Loss: 9.547216260874713\n",
      "Best model saved with loss 9.547216260874713 at epoch 15\n",
      "Epoch 16/1000, Loss: 9.551370404384754\n",
      "Epoch 17/1000, Loss: 9.546085370911491\n",
      "Best model saved with loss 9.546085370911491 at epoch 17\n",
      "Epoch 18/1000, Loss: 9.547566268179152\n",
      "Epoch 19/1000, Loss: 9.545391974625764\n",
      "Best model saved with loss 9.545391974625764 at epoch 19\n",
      "Epoch 20/1000, Loss: 9.545334250838668\n",
      "Best model saved with loss 9.545334250838668 at epoch 20\n",
      "Epoch 21/1000, Loss: 9.54447490197641\n",
      "Best model saved with loss 9.54447490197641 at epoch 21\n",
      "Epoch 22/1000, Loss: 9.544314759748953\n",
      "Best model saved with loss 9.544314759748953 at epoch 22\n",
      "Epoch 23/1000, Loss: 9.543720068754974\n",
      "Best model saved with loss 9.543720068754974 at epoch 23\n",
      "Epoch 24/1000, Loss: 9.54789767883442\n",
      "Epoch 25/1000, Loss: 9.542309006055197\n",
      "Best model saved with loss 9.542309006055197 at epoch 25\n",
      "Epoch 26/1000, Loss: 9.543620979344404\n",
      "Epoch 27/1000, Loss: 9.544265672012612\n",
      "Epoch 28/1000, Loss: 9.543499169526276\n",
      "Epoch 29/1000, Loss: 9.542250880488643\n",
      "Best model saved with loss 9.542250880488643 at epoch 29\n",
      "Epoch 30/1000, Loss: 9.542959893191302\n",
      "Epoch 31/1000, Loss: 9.543390719978898\n",
      "Epoch 32/1000, Loss: 9.543268565778378\n",
      "Epoch 33/1000, Loss: 9.541703162369904\n",
      "Best model saved with loss 9.541703162369904 at epoch 33\n",
      "Epoch 34/1000, Loss: 9.540609668802333\n",
      "Best model saved with loss 9.540609668802333 at epoch 34\n",
      "Epoch 35/1000, Loss: 9.542676082363835\n",
      "Epoch 36/1000, Loss: 9.541327216007092\n",
      "Epoch 37/1000, Loss: 9.540803048345778\n",
      "Epoch 38/1000, Loss: 9.543583658006456\n",
      "Epoch 39/1000, Loss: 9.541289943235892\n",
      "Epoch 40/1000, Loss: 9.54123001186936\n",
      "Epoch 41/1000, Loss: 9.540735054899145\n",
      "Epoch 42/1000, Loss: 9.54163956200635\n",
      "Epoch 43/1000, Loss: 9.539632351310164\n",
      "Best model saved with loss 9.539632351310164 at epoch 43\n",
      "Epoch 44/1000, Loss: 9.541579727773312\n",
      "Epoch 45/1000, Loss: 9.541000074810452\n",
      "Epoch 46/1000, Loss: 9.541336589389378\n",
      "Epoch 47/1000, Loss: 9.540568276687905\n",
      "Epoch 48/1000, Loss: 9.540079960116634\n",
      "Epoch 49/1000, Loss: 9.540032536895186\n",
      "Epoch 50/1000, Loss: 9.541258524965357\n",
      "Epoch 51/1000, Loss: 9.540968078154105\n",
      "Epoch 52/1000, Loss: 9.541208293702867\n",
      "Epoch 53/1000, Loss: 9.53953657326875\n",
      "Best model saved with loss 9.53953657326875 at epoch 53\n",
      "Epoch 54/1000, Loss: 9.539442552460564\n",
      "Best model saved with loss 9.539442552460564 at epoch 54\n",
      "Epoch 55/1000, Loss: 9.5385341335226\n",
      "Best model saved with loss 9.5385341335226 at epoch 55\n",
      "Epoch 56/1000, Loss: 9.539613869455126\n",
      "Epoch 57/1000, Loss: 9.538749990639863\n",
      "Epoch 58/1000, Loss: 9.540080119062353\n",
      "Epoch 59/1000, Loss: 9.540291406490185\n",
      "Epoch 60/1000, Loss: 9.540989677111307\n",
      "Epoch 61/1000, Loss: 9.539064354366726\n",
      "Epoch 62/1000, Loss: 9.539806944352609\n",
      "Epoch 63/1000, Loss: 9.54003119468689\n",
      "Epoch 64/1000, Loss: 9.539397054248386\n",
      "Epoch 65/1000, Loss: 9.540091744175664\n",
      "Epoch 66/1000, Loss: 9.539086597937125\n",
      "Epoch 67/1000, Loss: 9.54030626349979\n",
      "Epoch 68/1000, Loss: 9.539261363170764\n",
      "Epoch 69/1000, Loss: 9.53844604227278\n",
      "Best model saved with loss 9.53844604227278 at epoch 69\n",
      "Epoch 70/1000, Loss: 9.539246488500524\n",
      "Epoch 71/1000, Loss: 9.538867504508406\n",
      "Epoch 72/1000, Loss: 9.538595654346326\n",
      "Epoch 73/1000, Loss: 9.539251150908294\n",
      "Epoch 74/1000, Loss: 9.540308903764796\n",
      "Epoch 75/1000, Loss: 9.540958872547856\n",
      "Epoch 76/1000, Loss: 9.538712541262308\n",
      "Epoch 77/1000, Loss: 9.537808210761458\n",
      "Best model saved with loss 9.537808210761458 at epoch 77\n",
      "Epoch 78/1000, Loss: 9.538959997671622\n",
      "Epoch 79/1000, Loss: 9.539133191108704\n",
      "Epoch 80/1000, Loss: 9.53898189244447\n",
      "Epoch 81/1000, Loss: 9.539099075176098\n",
      "Epoch 82/1000, Loss: 9.539834689210963\n",
      "Epoch 83/1000, Loss: 9.538538305847734\n",
      "Epoch 84/1000, Loss: 9.539858389783788\n",
      "Epoch 85/1000, Loss: 9.53786019925718\n",
      "Epoch 86/1000, Loss: 9.540152108227765\n",
      "Epoch 87/1000, Loss: 9.538397696283129\n",
      "Epoch 88/1000, Loss: 9.54056234271438\n",
      "Epoch 89/1000, Loss: 9.539040194617378\n",
      "Epoch 90/1000, Loss: 9.539269632763332\n",
      "Epoch 91/1000, Loss: 9.538940261911463\n",
      "Epoch 92/1000, Loss: 9.538641028934055\n",
      "Epoch 93/1000, Loss: 9.538350727823046\n",
      "Epoch 94/1000, Loss: 9.540956347076982\n",
      "Epoch 95/1000, Loss: 9.537539380568045\n",
      "Best model saved with loss 9.537539380568045 at epoch 95\n",
      "Epoch 96/1000, Loss: 9.539586248221221\n",
      "Epoch 97/1000, Loss: 9.539197007815043\n",
      "Epoch 98/1000, Loss: 9.540239651997885\n",
      "Epoch 99/1000, Loss: 9.541019867967677\n",
      "Epoch 100/1000, Loss: 9.539170998114127\n",
      "Epoch 101/1000, Loss: 9.539501587549845\n",
      "Epoch 102/1000, Loss: 9.539002007908291\n",
      "Epoch 103/1000, Loss: 9.537538047190067\n",
      "Best model saved with loss 9.537538047190067 at epoch 103\n",
      "Epoch 104/1000, Loss: 9.538049123905322\n",
      "Epoch 105/1000, Loss: 9.538557613337481\n",
      "Epoch 106/1000, Loss: 9.540462635181568\n",
      "Epoch 107/1000, Loss: 9.538627500887271\n",
      "Epoch 108/1000, Loss: 9.538863535280582\n",
      "Epoch 109/1000, Loss: 9.537874888490748\n",
      "Epoch 110/1000, Loss: 9.536878011844776\n",
      "Best model saved with loss 9.536878011844776 at epoch 110\n",
      "Epoch 111/1000, Loss: 9.54045135445065\n",
      "Epoch 112/1000, Loss: 9.540356508007756\n",
      "Epoch 113/1000, Loss: 9.539021266831291\n",
      "Epoch 114/1000, Loss: 9.539543218082851\n",
      "Epoch 115/1000, Loss: 9.537827672781768\n",
      "Epoch 116/1000, Loss: 9.537988556755913\n",
      "Epoch 117/1000, Loss: 9.538617818443864\n",
      "Epoch 118/1000, Loss: 9.539735127378393\n",
      "Epoch 119/1000, Loss: 9.537587704481902\n",
      "Epoch 120/1000, Loss: 9.539569996021411\n",
      "Epoch 121/1000, Loss: 9.537899370546695\n",
      "Epoch 122/1000, Loss: 9.53848162403813\n",
      "Epoch 123/1000, Loss: 9.539810026133502\n",
      "Epoch 124/1000, Loss: 9.53963381714291\n",
      "Epoch 125/1000, Loss: 9.538305521011353\n",
      "Epoch 126/1000, Loss: 9.538466524194789\n",
      "Epoch 127/1000, Loss: 9.537314609245017\n",
      "Epoch 128/1000, Loss: 9.538812906653792\n",
      "Epoch 129/1000, Loss: 9.538509148138541\n",
      "Epoch 130/1000, Loss: 9.539991308141637\n",
      "Epoch 131/1000, Loss: 9.540590233272976\n",
      "Epoch 132/1000, Loss: 9.539745953347948\n",
      "Epoch 133/1000, Loss: 9.538717817377162\n",
      "Epoch 134/1000, Loss: 9.539981471167671\n",
      "Epoch 135/1000, Loss: 9.538603067398071\n",
      "Epoch 136/1000, Loss: 9.540009317574677\n",
      "Epoch 137/1000, Loss: 9.53846878917129\n",
      "Epoch 138/1000, Loss: 9.539108439728066\n",
      "Epoch 139/1000, Loss: 9.538272734041568\n",
      "Epoch 140/1000, Loss: 9.537870906017444\n",
      "Epoch 141/1000, Loss: 9.539222019690055\n",
      "Epoch 142/1000, Loss: 9.53755846729985\n",
      "Epoch 143/1000, Loss: 9.538921153103864\n",
      "Epoch 144/1000, Loss: 9.538884825176662\n",
      "Epoch 145/1000, Loss: 9.53863740408862\n",
      "Epoch 146/1000, Loss: 9.541176248479772\n",
      "Epoch 147/1000, Loss: 9.538346272927743\n",
      "Epoch 148/1000, Loss: 9.538412738729406\n",
      "Epoch 149/1000, Loss: 9.537871312212062\n",
      "Epoch 150/1000, Loss: 9.538217544555664\n",
      "Epoch 151/1000, Loss: 9.537294286268729\n",
      "Epoch 152/1000, Loss: 9.538638088438246\n",
      "Epoch 153/1000, Loss: 9.538538442717659\n",
      "Epoch 154/1000, Loss: 9.537975836683202\n",
      "Epoch 155/1000, Loss: 9.538979993926155\n",
      "Epoch 156/1000, Loss: 9.539786157784638\n",
      "Epoch 157/1000, Loss: 9.537890478416726\n",
      "Epoch 158/1000, Loss: 9.538735703185752\n",
      "Epoch 159/1000, Loss: 9.538797422691628\n",
      "Epoch 160/1000, Loss: 9.538728837613705\n",
      "Epoch 161/1000, Loss: 9.53805990130813\n",
      "Epoch 162/1000, Loss: 9.538638061947292\n",
      "Epoch 163/1000, Loss: 9.539014900172198\n",
      "Epoch 164/1000, Loss: 9.536668013643336\n",
      "Best model saved with loss 9.536668013643336 at epoch 164\n",
      "Epoch 165/1000, Loss: 9.538554699332625\n",
      "Epoch 166/1000, Loss: 9.538305507765877\n",
      "Epoch 167/1000, Loss: 9.53859813566561\n",
      "Epoch 168/1000, Loss: 9.538472197673938\n",
      "Epoch 169/1000, Loss: 9.538179357846579\n",
      "Epoch 170/1000, Loss: 9.538707547717625\n",
      "Epoch 171/1000, Loss: 9.53809044096205\n",
      "Epoch 172/1000, Loss: 9.53769298836037\n",
      "Epoch 173/1000, Loss: 9.53883961836497\n",
      "Epoch 174/1000, Loss: 9.538968885386431\n",
      "Epoch 175/1000, Loss: 9.537910064061483\n",
      "Epoch 176/1000, Loss: 9.538038125744572\n",
      "Epoch 177/1000, Loss: 9.537879316895097\n",
      "Epoch 178/1000, Loss: 9.539612076900623\n",
      "Epoch 179/1000, Loss: 9.537583426192954\n",
      "Epoch 180/1000, Loss: 9.538602343312016\n",
      "Epoch 181/1000, Loss: 9.53802137463181\n",
      "Epoch 182/1000, Loss: 9.539088761364972\n",
      "Epoch 183/1000, Loss: 9.540114826626247\n",
      "Epoch 184/1000, Loss: 9.537308949011344\n",
      "Epoch 185/1000, Loss: 9.537937195212752\n",
      "Epoch 186/1000, Loss: 9.538093734670568\n",
      "Epoch 187/1000, Loss: 9.539008771931684\n",
      "Epoch 188/1000, Loss: 9.536698849112899\n",
      "Epoch 189/1000, Loss: 9.536667152687356\n",
      "Best model saved with loss 9.536667152687356 at epoch 189\n",
      "Epoch 190/1000, Loss: 9.537452278313813\n",
      "Epoch 191/1000, Loss: 9.537360902185794\n",
      "Epoch 192/1000, Loss: 9.538966708713108\n",
      "Epoch 193/1000, Loss: 9.53911613976514\n",
      "Epoch 194/1000, Loss: 9.538166377279493\n",
      "Epoch 195/1000, Loss: 9.539203511344063\n",
      "Epoch 196/1000, Loss: 9.536794190053586\n",
      "Epoch 197/1000, Loss: 9.537536091274685\n",
      "Epoch 198/1000, Loss: 9.538549736694053\n",
      "Epoch 199/1000, Loss: 9.538108582849857\n",
      "Epoch 200/1000, Loss: 9.538788508485865\n",
      "Epoch 201/1000, Loss: 9.53866215546926\n",
      "Epoch 202/1000, Loss: 9.537706326555323\n",
      "Epoch 203/1000, Loss: 9.539564812624896\n",
      "Epoch 204/1000, Loss: 9.538731742788244\n",
      "Epoch 205/1000, Loss: 9.53773299853007\n",
      "Epoch 206/1000, Loss: 9.540200290856538\n",
      "Epoch 207/1000, Loss: 9.537406453379878\n",
      "Epoch 208/1000, Loss: 9.53905142236639\n",
      "Epoch 209/1000, Loss: 9.538846051251447\n",
      "Epoch 210/1000, Loss: 9.539326601558262\n",
      "Epoch 211/1000, Loss: 9.537418687785113\n",
      "Epoch 212/1000, Loss: 9.53743248074143\n",
      "Epoch 213/1000, Loss: 9.537536223729452\n",
      "Epoch 214/1000, Loss: 9.538660694051671\n",
      "Epoch 215/1000, Loss: 9.538224273257786\n",
      "Epoch 216/1000, Loss: 9.537409053908455\n",
      "Epoch 217/1000, Loss: 9.540523025724623\n",
      "Epoch 218/1000, Loss: 9.538653426700169\n",
      "Epoch 219/1000, Loss: 9.538668032045718\n",
      "Epoch 220/1000, Loss: 9.539290534125435\n",
      "Epoch 221/1000, Loss: 9.537301271050065\n",
      "Epoch 222/1000, Loss: 9.538765209692496\n",
      "Epoch 223/1000, Loss: 9.538360467663518\n",
      "Epoch 224/1000, Loss: 9.539198729727003\n",
      "Epoch 225/1000, Loss: 9.538109033196061\n",
      "Epoch 226/1000, Loss: 9.540015595930594\n",
      "Epoch 227/1000, Loss: 9.539134374371281\n",
      "Epoch 228/1000, Loss: 9.538874829256976\n",
      "Epoch 229/1000, Loss: 9.538284328248766\n",
      "Epoch 230/1000, Loss: 9.538205124713757\n",
      "Epoch 231/1000, Loss: 9.538827940269753\n",
      "Epoch 232/1000, Loss: 9.539447757932875\n",
      "Epoch 233/1000, Loss: 9.538279122776455\n",
      "Epoch 234/1000, Loss: 9.53864914841122\n",
      "Epoch 235/1000, Loss: 9.537378041832536\n",
      "Epoch 236/1000, Loss: 9.538605513396087\n",
      "Epoch 237/1000, Loss: 9.536994470490349\n",
      "Epoch 238/1000, Loss: 9.537786161458051\n",
      "Epoch 239/1000, Loss: 9.538741442892286\n",
      "Epoch 240/1000, Loss: 9.538855521767228\n",
      "Epoch 241/1000, Loss: 9.53907999727461\n",
      "Epoch 242/1000, Loss: 9.538850144103721\n",
      "Epoch 243/1000, Loss: 9.537826670540703\n",
      "Epoch 244/1000, Loss: 9.538800085032427\n",
      "Epoch 245/1000, Loss: 9.53621663429119\n",
      "Best model saved with loss 9.53621663429119 at epoch 245\n",
      "Epoch 246/1000, Loss: 9.537249114778307\n",
      "Epoch 247/1000, Loss: 9.537582830146507\n",
      "Epoch 248/1000, Loss: 9.538057066776135\n",
      "Epoch 249/1000, Loss: 9.53794553986302\n",
      "Epoch 250/1000, Loss: 9.538704540994432\n",
      "Epoch 251/1000, Loss: 9.539073197929948\n",
      "Epoch 252/1000, Loss: 9.538765805738944\n",
      "Epoch 253/1000, Loss: 9.537404863922685\n",
      "Epoch 254/1000, Loss: 9.53777218748022\n",
      "Epoch 255/1000, Loss: 9.538953401424267\n",
      "Epoch 256/1000, Loss: 9.537647852191219\n",
      "Epoch 257/1000, Loss: 9.538702774930883\n",
      "Epoch 258/1000, Loss: 9.539945902647796\n",
      "Epoch 259/1000, Loss: 9.53771388972247\n",
      "Epoch 260/1000, Loss: 9.539455082681444\n",
      "Epoch 261/1000, Loss: 9.537223758520904\n",
      "Epoch 262/1000, Loss: 9.538176302556638\n",
      "Epoch 263/1000, Loss: 9.536846381646615\n",
      "Epoch 264/1000, Loss: 9.539029218532422\n",
      "Epoch 265/1000, Loss: 9.537799963244685\n",
      "Epoch 266/1000, Loss: 9.538780857015539\n",
      "Epoch 267/1000, Loss: 9.538469853224578\n",
      "Epoch 268/1000, Loss: 9.53888812330034\n",
      "Epoch 269/1000, Loss: 9.538697927086442\n",
      "Epoch 270/1000, Loss: 9.538590625480369\n",
      "Epoch 271/1000, Loss: 9.538403299119738\n",
      "Epoch 272/1000, Loss: 9.537554714414808\n",
      "Epoch 273/1000, Loss: 9.537912872102526\n",
      "Epoch 274/1000, Loss: 9.538291096687317\n",
      "Epoch 275/1000, Loss: 9.53828067249722\n",
      "Epoch 276/1000, Loss: 9.536772648493448\n",
      "Epoch 277/1000, Loss: 9.539064994564763\n",
      "Epoch 278/1000, Loss: 9.538416840412\n",
      "Epoch 279/1000, Loss: 9.539058031859222\n",
      "Epoch 280/1000, Loss: 9.538773730949119\n",
      "Epoch 281/1000, Loss: 9.53956386336574\n",
      "Epoch 282/1000, Loss: 9.538029343993575\n",
      "Epoch 283/1000, Loss: 9.538779413258588\n",
      "Epoch 284/1000, Loss: 9.536990889796504\n",
      "Epoch 285/1000, Loss: 9.538576819278576\n",
      "Epoch 286/1000, Loss: 9.539601105230826\n",
      "Epoch 287/1000, Loss: 9.539025147755941\n",
      "Epoch 288/1000, Loss: 9.538438849978977\n",
      "Epoch 289/1000, Loss: 9.538627130013925\n",
      "Epoch 290/1000, Loss: 9.539229622593632\n",
      "Epoch 291/1000, Loss: 9.53738933580893\n",
      "Epoch 292/1000, Loss: 9.537775450282627\n",
      "Epoch 293/1000, Loss: 9.538193729188707\n",
      "Epoch 294/1000, Loss: 9.53878594327856\n",
      "Epoch 295/1000, Loss: 9.537600980864632\n",
      "Epoch 296/1000, Loss: 9.538357809737876\n",
      "Epoch 297/1000, Loss: 9.53721816892977\n",
      "Epoch 298/1000, Loss: 9.537820873437104\n",
      "Epoch 299/1000, Loss: 9.537124302652147\n",
      "Epoch 300/1000, Loss: 9.538333451306379\n",
      "Epoch 301/1000, Loss: 9.538104869701245\n",
      "Epoch 302/1000, Loss: 9.538548818341008\n",
      "Epoch 303/1000, Loss: 9.53821505440606\n",
      "Epoch 304/1000, Loss: 9.538277626037598\n",
      "Epoch 305/1000, Loss: 9.53915622499254\n",
      "Epoch 306/1000, Loss: 9.538497946880481\n",
      "Epoch 307/1000, Loss: 9.538558310932583\n",
      "Epoch 308/1000, Loss: 9.537752959463331\n",
      "Epoch 309/1000, Loss: 9.538187349284136\n",
      "Epoch 310/1000, Loss: 9.53905835416582\n",
      "Epoch 311/1000, Loss: 9.538278041062531\n",
      "Epoch 312/1000, Loss: 9.536690199816668\n",
      "Epoch 313/1000, Loss: 9.539445894735831\n",
      "Epoch 314/1000, Loss: 9.539599061012268\n",
      "Epoch 315/1000, Loss: 9.537452340126038\n",
      "Epoch 316/1000, Loss: 9.538371947076586\n",
      "Epoch 317/1000, Loss: 9.538951524981746\n",
      "Epoch 318/1000, Loss: 9.539655791388618\n",
      "Epoch 319/1000, Loss: 9.537242668646353\n",
      "Epoch 320/1000, Loss: 9.537346380728263\n",
      "Epoch 321/1000, Loss: 9.538466175397238\n",
      "Epoch 322/1000, Loss: 9.538527824260571\n",
      "Epoch 323/1000, Loss: 9.53760649981322\n",
      "Epoch 324/1000, Loss: 9.538195627707022\n",
      "Epoch 325/1000, Loss: 9.53857265578376\n",
      "Epoch 326/1000, Loss: 9.536870806305497\n",
      "Epoch 327/1000, Loss: 9.536978933546278\n",
      "Epoch 328/1000, Loss: 9.538603164531567\n",
      "Epoch 329/1000, Loss: 9.537419848971897\n",
      "Epoch 330/1000, Loss: 9.537568048194602\n",
      "Epoch 331/1000, Loss: 9.536848169785959\n",
      "Epoch 332/1000, Loss: 9.53892386842657\n",
      "Epoch 333/1000, Loss: 9.5379143776717\n",
      "Epoch 334/1000, Loss: 9.538344824755633\n",
      "Epoch 335/1000, Loss: 9.537470508504796\n",
      "Epoch 336/1000, Loss: 9.53756731969339\n",
      "Epoch 337/1000, Loss: 9.539445766696224\n",
      "Epoch 338/1000, Loss: 9.537850234243605\n",
      "Epoch 339/1000, Loss: 9.538338087223194\n",
      "Epoch 340/1000, Loss: 9.536925858921474\n",
      "Epoch 341/1000, Loss: 9.537830626523053\n",
      "Epoch 342/1000, Loss: 9.537883718808493\n",
      "Epoch 343/1000, Loss: 9.539248802043774\n",
      "Epoch 344/1000, Loss: 9.538195119963753\n",
      "Epoch 345/1000, Loss: 9.537488716619986\n",
      "Epoch 346/1000, Loss: 9.537607921494377\n",
      "Epoch 347/1000, Loss: 9.537390254161975\n",
      "Epoch 348/1000, Loss: 9.538235156624406\n",
      "Epoch 349/1000, Loss: 9.539114943257085\n",
      "Epoch 350/1000, Loss: 9.540163530243767\n",
      "Epoch 351/1000, Loss: 9.538839609534651\n",
      "Epoch 352/1000, Loss: 9.538111011187235\n",
      "Epoch 353/1000, Loss: 9.537532214765195\n",
      "Epoch 354/1000, Loss: 9.53999490208096\n",
      "Epoch 355/1000, Loss: 9.538627169750354\n",
      "Epoch 356/1000, Loss: 9.537371578039947\n",
      "Epoch 357/1000, Loss: 9.539472703580502\n",
      "Epoch 358/1000, Loss: 9.537665398032576\n",
      "Epoch 359/1000, Loss: 9.535729390603525\n",
      "Best model saved with loss 9.535729390603525 at epoch 359\n",
      "Epoch 360/1000, Loss: 9.5366174115075\n",
      "Epoch 361/1000, Loss: 9.538009599403098\n",
      "Epoch 362/1000, Loss: 9.537578300193504\n",
      "Epoch 363/1000, Loss: 9.54035379268505\n",
      "Epoch 364/1000, Loss: 9.538580885639897\n",
      "Epoch 365/1000, Loss: 9.53745260062041\n",
      "Epoch 366/1000, Loss: 9.53882172372606\n",
      "Epoch 367/1000, Loss: 9.537696184935394\n",
      "Epoch 368/1000, Loss: 9.53815167038529\n",
      "Epoch 369/1000, Loss: 9.538880939836856\n",
      "Epoch 370/1000, Loss: 9.537781812526562\n",
      "Epoch 371/1000, Loss: 9.537708631268254\n",
      "Epoch 372/1000, Loss: 9.53900178273519\n",
      "Epoch 373/1000, Loss: 9.538645059974105\n",
      "Epoch 374/1000, Loss: 9.537680338930201\n",
      "Epoch 375/1000, Loss: 9.538485279789677\n",
      "Epoch 376/1000, Loss: 9.538002791228118\n",
      "Epoch 377/1000, Loss: 9.537416374241865\n",
      "Epoch 378/1000, Loss: 9.537094668105796\n",
      "Epoch 379/1000, Loss: 9.538424672903838\n",
      "Epoch 380/1000, Loss: 9.537947959370083\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16096/1780229412.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;31m# Call the training function with TensorBoard logging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m \u001b[0mbest_model_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_autoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Best model saved at: {best_model_path}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16096/1780229412.py\u001b[0m in \u001b[0;36mtrain_autoencoder\u001b[1;34m(model, train_loader, device, epochs, lr, log_dir)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m  \u001b[1;31m# Node features for all graphs in the batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0medge_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m  \u001b[1;31m# Edge indices for all graphs in the batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Projects\\DiffusionPointClouds\\point_cloud_datasest.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Create edge_index using k-nearest neighbors (kNN)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0medge_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknn_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoint_cloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# shape: [2, num_edges]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Compute Laplacian (sparse form using edge weights)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\torch_geometric\\nn\\pool\\__init__.py\u001b[0m in \u001b[0;36mknn_graph\u001b[1;34m(x, k, batch, loop, flow, cosine, num_workers)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0medge_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknn_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \"\"\"\n\u001b[1;32m--> 130\u001b[1;33m     return torch_cluster.knn_graph(x, k, batch, loop, flow, cosine,\n\u001b[0m\u001b[0;32m    131\u001b[0m                                    num_workers)\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\torch_cluster\\knn.py\u001b[0m in \u001b[0;36mknn_graph\u001b[1;34m(x, k, batch, loop, flow, cosine, num_workers, batch_size)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mflow\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'source_to_target'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'target_to_source'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m     edge_index = knn(x, x, k if loop else k + 1, batch, batch, cosine,\n\u001b[0m\u001b[0;32m    133\u001b[0m                      num_workers, batch_size)\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\torch_cluster\\knn.py\u001b[0m in \u001b[0;36mknn\u001b[1;34m(x, y, k, batch_x, batch_y, cosine, num_workers, batch_size)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mptr_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbucketize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     return torch.ops.torch_cluster.knn(x, y, ptr_x, ptr_y, k, cosine,\n\u001b[0m\u001b[0;32m     82\u001b[0m                                        num_workers)\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1059\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_torchbind_op_overload\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_call_overload_packet_from_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m     \u001b[1;31m# TODO: use this to make a __dir__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_autoencoder(model, train_loader, device, epochs=1000, lr=1e-3, log_dir=\"runs/autoencoder\"):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_path = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            x = batch.x  # Node features for all graphs in the batch\n",
    "            edge_index = batch.edge_index  # Edge indices for all graphs in the batch\n",
    "            edge_weight = batch.edge_weight  # Edge weights (Laplacian) for all graphs in the batch\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move data to the specified device (GPU or CPU)\n",
    "            x = x.to(device)  # torch.Size([128, 1024, 3])\n",
    "            edge_index = edge_index.to(device)  # torch.Size([128, 2, 6144])\n",
    "            edge_weight = edge_weight.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            reconstructed = model(x, edge_index, edge_weight=edge_weight)\n",
    "            loss = loss_fn(reconstructed, x)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss for the epoch\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Log batch-level loss to TensorBoard\n",
    "            writer.add_scalar('Loss/train_batch', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "        # Average loss per epoch\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Log epoch-level loss to TensorBoard\n",
    "        writer.add_scalar('Loss/train_epoch', avg_epoch_loss, epoch)\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch}/{epochs}, Loss: {avg_epoch_loss}')\n",
    "\n",
    "        # Check if the current model is the best one (based on loss)\n",
    "        if avg_epoch_loss < best_loss:\n",
    "            best_loss = avg_epoch_loss\n",
    "            best_model_path = f\"models/best_model_epoch_{epoch}.pth\"\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Best model saved with loss {best_loss} at epoch {epoch}\")\n",
    "\n",
    "    # Close TensorBoard writer when training is complete\n",
    "    writer.close()\n",
    "\n",
    "    # Return the path to the best model for further usage\n",
    "    return best_model_path\n",
    "\n",
    "# Initialize and train the model\n",
    "hidden_features = 64\n",
    "latent_dim = 32\n",
    "\n",
    "model = DiffusionNetAutoencoder(POINT_DIM, hidden_features, latent_dim).to(device)\n",
    "\n",
    "# Call the training function with TensorBoard logging\n",
    "best_model_path = train_autoencoder(model, train_loader, device)\n",
    "print(f\"Best model saved at: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.0, 7.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"get_laplacian: IndexError: The shape of the mask [2, 6144] at index 0 does not match the shape of the indexed tensor [128, 2, 7168] at index 0\"\n",
    "\"norm_laplacian: IndexError: The shape of the mask [2, 6144] at index 0 does not match the shape of the indexed tensor [128, 1024, 1024] at index 0\"\n",
    "6144/1024, 7168/1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
