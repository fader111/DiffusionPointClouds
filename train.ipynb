{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.4.0+cu124', 'cuda available')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import ChebConv  # Using Chebyshev convolution\n",
    "from torch_geometric.nn import knn_graph\n",
    "from torch_geometric.utils import get_laplacian, get_mesh_laplacian\n",
    "# from torch_sparse import coalesce\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "from embedder_dataset import EmbedderDataset\n",
    "# from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "from models import *\n",
    "from point_cloud_datasest import PointCloudDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.__version__, \"cuda available\" if torch.cuda.is_available() else \"cpu only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\5010858\\AppData\\Local\\Temp/ipykernel_17840/2273564222.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ds = torch.load(ds_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device, ds length - 27571, data.shape - (27571, 3072)\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "\n",
    "# ds_dir = \"datasets_embedded\"\n",
    "# ds_fname = \"ds_1024.pth\" # 1024 points per shape\n",
    "ds_path = r\"C:\\Projects\\multiSetup\\datasets_embedded\\ds_1024.pth\"\n",
    "ds = torch.load(ds_path)\n",
    "dataset_data = ds.data\n",
    "\n",
    "print(f\"Using {device} device, ds length - {len(ds)}, data.shape - {dataset_data.shape}\")\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "SPLIT_FACTOR = .8\n",
    "TRAIN_MODE = (False, True)[1]\n",
    "REMOVE_OLD_MODELS = True\n",
    "POINTS_PER_SHAPE = 1024\n",
    "POINT_DIM = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "diffision_ae_ds = PointCloudDataset(torch.from_numpy(dataset_data), k=6)\n",
    "len(diffision_ae_ds)\n",
    "train_loader = DataLoader(diffision_ae_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_loader))\n",
    "# batch # DataBatch(x=[131072, 3], edge_index=[2, 786432], edge_weight=[786432], batch=[131072], ptr=[129])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Loss: 24.7296641446926\n",
      "Best model saved with loss 24.7296641446926 at epoch 0\n",
      "Epoch 1/1000, Loss: 3.1331687757262476\n",
      "Best model saved with loss 3.1331687757262476 at epoch 1\n",
      "Epoch 2/1000, Loss: 1.7297658202824768\n",
      "Best model saved with loss 1.7297658202824768 at epoch 2\n",
      "Epoch 3/1000, Loss: 1.3079000385823074\n",
      "Best model saved with loss 1.3079000385823074 at epoch 3\n",
      "Epoch 4/1000, Loss: 1.0447344837917223\n",
      "Best model saved with loss 1.0447344837917223 at epoch 4\n",
      "Epoch 5/1000, Loss: 0.8376272643605868\n",
      "Best model saved with loss 0.8376272643605868 at epoch 5\n",
      "Epoch 6/1000, Loss: 0.6075225901548509\n",
      "Best model saved with loss 0.6075225901548509 at epoch 6\n",
      "Epoch 7/1000, Loss: 0.5261157759361796\n",
      "Best model saved with loss 0.5261157759361796 at epoch 7\n",
      "Epoch 8/1000, Loss: 0.4561961189740234\n",
      "Best model saved with loss 0.4561961189740234 at epoch 8\n",
      "Epoch 9/1000, Loss: 0.42252739643057186\n",
      "Best model saved with loss 0.42252739643057186 at epoch 9\n",
      "Epoch 10/1000, Loss: 0.3701668699030523\n",
      "Best model saved with loss 0.3701668699030523 at epoch 10\n",
      "Epoch 11/1000, Loss: 0.327612994859616\n",
      "Best model saved with loss 0.327612994859616 at epoch 11\n",
      "Epoch 12/1000, Loss: 0.29375133463354025\n",
      "Best model saved with loss 0.29375133463354025 at epoch 12\n",
      "Epoch 13/1000, Loss: 0.27837898540827966\n",
      "Best model saved with loss 0.27837898540827966 at epoch 13\n",
      "Epoch 14/1000, Loss: 0.24101221354471314\n",
      "Best model saved with loss 0.24101221354471314 at epoch 14\n",
      "Epoch 15/1000, Loss: 0.2283537626542427\n",
      "Best model saved with loss 0.2283537626542427 at epoch 15\n",
      "Epoch 16/1000, Loss: 0.21254115910441787\n",
      "Best model saved with loss 0.21254115910441787 at epoch 16\n",
      "Epoch 17/1000, Loss: 0.2001160911663815\n",
      "Best model saved with loss 0.2001160911663815 at epoch 17\n",
      "Epoch 18/1000, Loss: 0.19320012139225448\n",
      "Best model saved with loss 0.19320012139225448 at epoch 18\n",
      "Epoch 19/1000, Loss: 0.1755494049715775\n",
      "Best model saved with loss 0.1755494049715775 at epoch 19\n",
      "Epoch 20/1000, Loss: 0.16984334312103413\n",
      "Best model saved with loss 0.16984334312103413 at epoch 20\n",
      "Epoch 21/1000, Loss: 0.19627376407798794\n",
      "Epoch 22/1000, Loss: 0.24658152536937483\n",
      "Epoch 23/1000, Loss: 0.7482057320023024\n",
      "Epoch 24/1000, Loss: 1.516055156580276\n",
      "Epoch 25/1000, Loss: 0.5788143001910713\n",
      "Epoch 26/1000, Loss: 0.6822408212831726\n",
      "Epoch 27/1000, Loss: 0.532582457418795\n",
      "Epoch 28/1000, Loss: 1.3766354310705706\n",
      "Epoch 29/1000, Loss: 0.24722611386742857\n",
      "Epoch 30/1000, Loss: 0.17236081510782242\n",
      "Epoch 31/1000, Loss: 0.13307665985215594\n",
      "Best model saved with loss 0.13307665985215594 at epoch 31\n",
      "Epoch 32/1000, Loss: 0.10637065409510224\n",
      "Best model saved with loss 0.10637065409510224 at epoch 32\n"
     ]
    }
   ],
   "source": [
    "def train_autoencoder(model, train_loader, device, epochs=1000, lr=1e-3, log_dir=\"runs/autoencoder\"):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_path = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            x = batch.x  # Node features for all graphs in the batch\n",
    "            edge_index = batch.edge_index  # Edge indices for all graphs in the batch\n",
    "            edge_weight = batch.edge_weight  # Edge weights (Laplacian) for all graphs in the batch\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move data to the specified device (GPU or CPU)\n",
    "            x = x.to(device)  # torch.Size([128, 1024, 3])\n",
    "            edge_index = edge_index.to(device)  # torch.Size([128, 2, 6144])\n",
    "            edge_weight = edge_weight.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            reconstructed, _ = model(x, edge_index, edge_weight=edge_weight)\n",
    "            loss = loss_fn(reconstructed, x)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss for the epoch\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Log batch-level loss to TensorBoard\n",
    "            writer.add_scalar('Loss/train_batch', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "        # Average loss per epoch\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Log epoch-level loss to TensorBoard\n",
    "        writer.add_scalar('Loss/train_epoch', avg_epoch_loss, epoch)\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch}/{epochs}, Loss: {avg_epoch_loss}')\n",
    "\n",
    "        # Check if the current model is the best one (based on loss)\n",
    "        if avg_epoch_loss < best_loss:\n",
    "            best_loss = avg_epoch_loss\n",
    "            best_model_path = f\"models/best_model_epoch_{epoch}.pth\"\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Best model saved with loss {best_loss} at epoch {epoch}\")\n",
    "\n",
    "    # Close TensorBoard writer when training is complete\n",
    "    writer.close()\n",
    "\n",
    "    # Return the path to the best model for further usage\n",
    "    return best_model_path\n",
    "\n",
    "# Initialize and train the model\n",
    "hidden_features = 64\n",
    "latent_dim = 32\n",
    "\n",
    "model = DiffusionNetAutoencoder(POINT_DIM, hidden_features, latent_dim).to(device)\n",
    "\n",
    "# Call the training function with TensorBoard logging\n",
    "best_model_path = train_autoencoder(model, train_loader, device)\n",
    "print(f\"Best model saved at: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.0, 7.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"get_laplacian: IndexError: The shape of the mask [2, 6144] at index 0 does not match the shape of the indexed tensor [128, 2, 7168] at index 0\"\n",
    "\"norm_laplacian: IndexError: The shape of the mask [2, 6144] at index 0 does not match the shape of the indexed tensor [128, 1024, 1024] at index 0\"\n",
    "6144/1024, 7168/1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
