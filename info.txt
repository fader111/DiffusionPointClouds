- сделал автоэнкодер для представления зубов - внутренне пространство предположительно 256* 32
- далее надо делать выравнивающую сеть, метки для которой - выход кодера сети зубов в T2, а входные данные - выход кодера этой же сети в T1
- для тренинга все есть, но для валидации нужен чистый декодер со своими сохраненными отдельными весами. 
    - надо сеть переделать так, что бы можно было сохранять веса отдельно кодера и отдельно декодера. model.encoder.save_state_dict, model.decoder.save_state_dict
        - upd ничего не надо переделывать. можно у лучшей модели сделать model.decoder.save_state_dict

теперь:
- запустить тренинг сейчас на пару эпох, остановить потом оставить учиться на ночь. 
- убавить эпохи - сделать 500 эпох чтоб за ночь доучилось.
- сделать модель полносвязного выравнивающего автоэнкодера и пропробовать тестовую учебу запустить. тестовая учеба - программа максимум. 
    - зубы приедут в формате с нулевыми коодинатами их надо будет рт в координаты головы, как я это уже делал при тренировке зубного энкодера. 

- сделать датасет для этой модели, для этого:
    - выход энкодера зуба 256*32 должен быть размещен во входном тензоре правильно - сначала нижняя, потом верхняя, 
        от 38 до 48, потом нижняя - 18 до 28. этот функционал есь в json_data_parser, но из json'а оттуда берутся только лендмарки, поэтому его надо переименовать в дата позиционер. 
    - данные на вход датасет креатора - это stl файлы мешей. 
        - мы к ним применяем ригид трансформы, которые берем из json файлов, (и этот функционал есть в validation.ipynb)
            причем для из ригид трансформов T1 делаем X а из T2 - метки
    - логика работы:
        - делаем модели кодера и декодера, переводим их в eval. (gpu или cpu?)
        - на лету скачиваем json файл кейса
        - вычисляем трансформы зубов
        - подгружаем stl файлы этого кейса, 
        - применяем трансформы, делаем облака точек, делаем далее то же что в PointCloudDataset
    - останавливается на PointCloudDataset knn - говорит tracked array не может иметь аттрибут numel
        проверять как это работает при валидации. - а валидации это тензор потомучто
    - семплер trimesh.sample.sample_surface_even иногда выдает ошибку. надо промодерировать. 
    - датасет размером больше 300 вызывает переполнение памяти, надо разбивать на части, потом конкатенировать его. 
        - конкатенированный датасет не имеет поля data. ну и бох с ним

        
- сделать валидацию модели